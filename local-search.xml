<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Test</title>
    <link href="/2025/04/06/Test/"/>
    <url>/2025/04/06/Test/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>考研结束</title>
    <link href="/2025/04/02/%E8%80%83%E7%A0%94%E7%BB%93%E6%9D%9F/"/>
    <url>/2025/04/02/%E8%80%83%E7%A0%94%E7%BB%93%E6%9D%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="我的考研历程"><a href="#我的考研历程" class="headerlink" title="我的考研历程"></a>我的考研历程</h1><h2 id="决定开始"><a href="#决定开始" class="headerlink" title="决定开始"></a>决定开始</h2><p>刚进大学时，我就了解到我们学校（cuit）没有保研，只有考研，同时也知道了408。当时，我误以为自己会考这几本书，所以在操作系统、计算机网络等课程上学习得特别认真，成绩也都还不错。直到大三上学期（2023年9月），我才真正明确了自己会走考研这条路。大三上学期是我整个大学最轻松的时光，没有竞赛、没有课设，也没有其他事情。每天只做三件事：吃饭、上课、健身。偶尔有心情时，就看看数学，因为12月要考六级，所以一直在背单词（背了很多，但还是没过，有些遗憾）。就这样，我度过了大学生涯中最惬意的一个学期。</p><h2 id="初试（2024-02-2024-12）"><a href="#初试（2024-02-2024-12）" class="headerlink" title="初试（2024.02 - 2024.12）"></a>初试（2024.02 - 2024.12）</h2><p>寒假期间，我开始稍微多学一点（但真的只多了一点），还心血来潮地去新都图书馆学习。虽然每天学习时间不多，但好歹也完成了任务——把武忠祥的高数基础学完了。那时我已经了解到408，但不太想学，这也为我后面选择自命题的学校埋下了伏笔。</p><p>寒假结束后，三月回到学校，我开始认真准备考研。当时还不确定要考哪里，只是先着手学习数据结构。每天上午没课或者有课可以逃的话，我就去图书馆做数学题。记得下午早些时候，图书馆五楼靠窗的位置阳光特别强烈，有时照得我眼睛都睁不开，但仍然不影响我喜欢坐在那里。</p><p>四月，我花了半天时间研究选择学校的事情，看了几所自命题的学校，大概了解了可能会考的内容，心里有了些底。五月特别忙，除了考研学习，还要准备课设代码、课设PPT，同时还要参加蓝桥杯、CSP考试，学习得有些稀里糊涂的。</p><p>六七八月，我每天都风雨无阻地去自习室学习（中间偷偷溜了5天去南宁看演唱会，哈哈）。这段时间最大的感触就是，每天像机器人一样，不断地刷题、改题、听课，累了就小憩一会儿，晚上想放松时就去远一点的地方吃饭，华联和另一边的商场是我们常去的地方。每天晚上7点，外面就有大妈们跳舞，声音很吵，只能戴着耳机学习。对了，早上我必须吃麦当劳，还买了两个月的早餐卡，感觉很不错。</p><p>九月开学后，我回到学校，一开始就去图书馆六楼靠窗的位置抢到了一张桌子，还和同学ltx一起买了小风扇、台灯、排插，感觉很惬意。印象很深的是，刚开学的九月份，我们还没找到图书馆的空位（六楼白天太阳晒得太热，当时只有晚上能去），最尴尬的是有空的地方基本上都没有空调，热得我难受极了。我跟ltx经常到处找位置，有一天，我们在下面学得很不舒服，我背上全是汗，特别想去六楼我们的位置。当时突然下了一场大暴雨，雨特别大，然后我就开心地跟他说：“我们可以去六楼了。”站在六楼窗户边，风吹得特别凉爽，我当时就想，这就像考研一样，是不是久逢甘霖呢？</p><p>后来，我了解到同学lyy和我考同一个学院，于是我们三人就一起学习了。lyy问我数据结构的问题，ltx问我数学的问题，大家一起交流问题，感觉很不错。报名后一百天，我们还进行了白日宣誓，哈哈，当时觉得特别尴尬。</p><p>直到考前一个月，我选择回家学习，把所有东西都搬回家里，在饭桌上摆好台灯和电脑。在家里，吃饭很快，睡觉也很舒服，每天晚上还和ltx打打游戏，交流一下学习情况。直到肖四、肖八出来以后，我总结了答案就开始背诵，特别喜欢在阳台边走边背，因为坐着腰会痛。背好了就去我爸面前给他背一下，英语作文也是这样背的。直到考试前，所有要背的内容都背好了，肖四所有大题也都背下来了。</p><p>考试那天，我去西南石油大学附近住了一个酒店，在酒店里主要就是背背政治和英语，很快就上考场了。考政治时，发现很多题目都没见过，最后改了一堆选择题，对答案才发现基本上都改错了（真的，最后别轻易改答案）。考英语时，阅读部分读得我满头大汗，手脚冰凉，作文更是写得不顺手。数学和数据结构考得还不错，大部分题目都能做，只是怕不仔细算错，好在该拿的分大部分都能做对。考完研后，我通宵了一晚上，报复性地玩耍。之后回到家对了答案，大概能考330分左右，心里很忐忑，担心今年分数线涨一些就麻烦了。</p><h2 id="复试（2025-01-2025-3）"><a href="#复试（2025-01-2025-3）" class="headerlink" title="复试（2025.01 - 2025.3）"></a>复试（2025.01 - 2025.3）</h2><p>初试结束后，我完全不想学习，只看了一点离散数学，完全没心情学，看到就害怕自己考不上。哎，我就是那种又懒又焦虑的人。</p><p>成绩出来后，我很激动，比预估高了30多分，这个分数应该能进复试。之后我又回到学校开始学习，但刚开始学的时候，我真的很迷茫，没有目标，不知道该看什么、学什么。后来买了红果研的资料，听了学长分享经历后，才稍微好一些，自己也理清楚了要学什么、怎么学。我在小红书（xhs）上找学长咨询，还被骗了200元，不过无所谓，这种人注定不能完成自己的理想。遇到不会的问题，我就问AI，然后自己一句一句查资料，总算把内容学得差不多了，但还是很忐忑，没办法检验自己的学习成果。考前一个星期，我又回家学习，每天看看书、做做题、准备资料，就这样一直到去长沙考试前。</p><p>在酒店里，虽然任务量比初试小，但还是非常紧张。考试那天上午，饭也没怎么吃，还有点拉肚子。笔试的离散数学部分都还好，都是我复习到的内容，只要仔细就好。操作系统有一道大题我不会，我也认了。计算机系统的浮点数计算，我居然错了，而且是最后两分钟才发现自己错了，已经无力回天。好在汇编语言部分很简单，难度不到我平时练习题难度的一半。</p><p>第二天面试，上午我一点东西都吃不下去，吃了就想吐。我的面试顺序比较靠前，是第四个，不过也好。有个考官一直问self-attention的内容，最后脑子一不清醒，回答错了，人家还提醒我下来再看看。还有一个老师问我gnn的问题，我没有做过相关研究，其他问题其实都还好，我觉得每个人对问题都有自己的理解，我就说了我的理解，但当时特别紧张，表述得都不太清楚。</p><p>面试结束后，我感觉自己整个复试发挥得不太好，也没有办法了。我提前联系好了导师，第二天下午去见了他，之后又回到酒店看看导师的论文。等到第二天，我去跟导师见面了，聊得其实还不错，导师说只要我能过线，就愿意要我。嗯，这已经是最好的结局了，当天晚上十点我就乘飞机回成都了。</p><h2 id="考研结束"><a href="#考研结束" class="headerlink" title="考研结束"></a>考研结束</h2><p>今天是四月二日，拟录取名单出来了，我的总排名第七，甚至比我初试排名还前进了两名。导师也给我加了微信，并且给我布置了一些任务。今天我还收到了另外两位导师的电话，他们都是当时面试我的老师，觉得我表现还不错，问我愿不愿意去他们那里。但我已经提前联系了现在的导师，所以就向他们说明了情况并表示感谢。</p><p>结果还算不错吧，考研结束了。现在该去忙毕业设计、毕业论文，以及导师布置的任务了。</p>]]></content>
    
    
    <categories>
      
      <category>Experience</category>
      
    </categories>
    
    
    <tags>
      
      <tag>考研</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数模心得之数维杯</title>
    <link href="/2024/01/26/%E6%95%B0%E6%A8%A1%E5%BF%83%E5%BE%97%E4%B9%8B%E6%95%B0%E7%BB%B4%E6%9D%AF/"/>
    <url>/2024/01/26/%E6%95%B0%E6%A8%A1%E5%BF%83%E5%BE%97%E4%B9%8B%E6%95%B0%E7%BB%B4%E6%9D%AF/</url>
    
    <content type="html"><![CDATA[<h2 id="数模心得之数维杯"><a href="#数模心得之数维杯" class="headerlink" title=" 数模心得之数维杯 "></a><center> 数模心得之数维杯 </center></h2><p>总算是没有寄的那么厉害的一次数模了原地哭泣www…<br><img src="/../pic/mathModeling/mathModeling2.png" alt="比赛结果"><br>下面来总结一下哪些做的好的地方和还可以再改进的地方</p><h2 id="好的地方"><a href="#好的地方" class="headerlink" title="好的地方"></a>好的地方</h2><p>1、吸取了前几次的教训，首先就是速度上一定不能拖，能开始的动手写论文就一定要开始写。</p><p>2、对于细节一定要把控好，精选到每个图片里面的每一个字母，排版什么的一定要留时间去检查。</p><p>3、网上的一些参考资料那就是参考，千万别认为他就是对的，一定要有自己的认知和判断。</p><p>4、遇到瓶颈的时候去看看有没有类似问法的往年优秀论文，解题思路就可以借鉴（比如这次的第四题就是看了往年国赛的一道题）。</p><p>5、有些参考资料是真的有用，去各大论文网站上找到有相似研究的，不仅仅让论文更加严谨而且写的也会很通畅</p><p><strong><font color=IndianRed>最后强调！细节！细节！细节！很重要</font></strong></p><h2 id="改进的方面"><a href="#改进的方面" class="headerlink" title="改进的方面"></a>改进的方面</h2><p>1、翻译时间一定要留够，当时以为翻译不会化太多时间，结果搞到凌晨5点。</p><p>2、写的时候忽略了很多小细节，比如答案非常不合理，和图片描述有问题，这些都是放到最后的最后才找出来问题所在。</p><p>3、还是经验有点不够，做题会存在很多不确定的地方。</p>]]></content>
    
    
    <categories>
      
      <category>Experience</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math modeling</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习中静态图动态图</title>
    <link href="/2023/12/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E9%9D%99%E6%80%81%E5%9B%BE%E5%8A%A8%E6%80%81%E5%9B%BE/"/>
    <url>/2023/12/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E9%9D%99%E6%80%81%E5%9B%BE%E5%8A%A8%E6%80%81%E5%9B%BE/</url>
    
    <content type="html"><![CDATA[<h2 id="相关知识"><a href="#相关知识" class="headerlink" title=" 相关知识 "></a><center> 相关知识 </center></h2><p>目前神经网络框架分为静态图框架和动态图框架，PyTorch 和 TensorFlow、Caffe 等框架最大的区别就是他们拥有不同的计算图表现形式。 TensorFlow 使用静态图，这意味着我们先定义计算图，然后不断使用它，而在 PyTorch 中，每次都会重新构建一个新的计算图。通过这次课程，我们会了解静态图和动态图之间的优缺点。</p><p>对于使用者来说，两种形式的计算图有着非常大的区别，同时静态图和动态图都有他们各自的优点，比如动态图比较方便debug，使用者能够用任何他们喜欢的方式进行debug，同时非常直观，而静态图是通过先定义后运行的方式，之后再次运行的时候就不再需要重新构建计算图，所以速度会比动态图更快。</p><p><strong>动态图</strong>：在计算的时候，就会把计算过程动态图存储起来。每次前向过程都会重新建立一个新图。</p><p><strong>静态图</strong>：静态图需要预先定义好运算规则流程，然后把运算流程存储下来。</p><p>两者的区别用一句话概括就是：</p><ul><li>动态图：运算与搭建同时进行；灵活，易调节。</li><li>静态图：先搭建图，后运算；高效，不灵活。</li></ul><p><img src="/../pic/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E9%9D%99%E6%80%81%E5%9B%BE%E4%B8%8E%E5%8A%A8%E6%80%81%E5%9B%BE/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E9%9D%99%E6%80%81%E5%9B%BE%E5%8A%A8%E6%80%81%E5%9B%BE1.gif" alt="演示动画"></p><h2 id="Pytorch中的动态图"><a href="#Pytorch中的动态图" class="headerlink" title="Pytorch中的动态图 "></a><center>Pytorch中的动态图 </center></h2><p>动态图的初步推导：</p><ul><li>计算图是用来描述运算的有向无环图</li><li>计算图有两个主要元素：结点（Node）和边（Edge）；</li><li>结点表示数据 ，如向量、矩阵、张量;</li><li>边表示运算 ，如加减乘除卷积等；</li></ul><p><img src="/../pic/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E9%9D%99%E6%80%81%E5%9B%BE%E4%B8%8E%E5%8A%A8%E6%80%81%E5%9B%BE/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E9%9D%99%E6%80%81%E5%9B%BE%E5%8A%A8%E6%80%81%E5%9B%BE2.png" alt="运算逻辑"></p><p>上图是用计算图表示：</p><p><strong><center> y &#x3D; ( x + w ) ∗ ( w + 1 ) </center></strong></p><p>其中, <code>a=x+w,b=w+1,y=a∗b</code>，(a和b是中间变量)，Pytorch在计算的时候，就会把计算过程用上面那样的动态图存储起来。现在我们计算一下y关于w的梯度：<code>∂y/∂w = ∂y/∂a * ∂a/∂w + ∂y/∂b * ∂b/∂w = b * 1 + a * 1 = b + a = x + w + w + 1 = 2w + x + 1</code>。</p><p>用Pytorch的代码来实现这个过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">w = torch.tensor([<span class="hljs-number">1.</span>],requires_grad = <span class="hljs-literal">True</span>)<br>x = torch.tensor([<span class="hljs-number">2.</span>],requires_grad = <span class="hljs-literal">True</span>)<br><br>a = w+x<br>b = w+<span class="hljs-number">1</span><br>y = a*b<br><br>y.backward()<br><span class="hljs-built_in">print</span>(w.grad)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">输出：</span><br><span class="hljs-string">tensor([5.])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>图中的叶子节点，是<code>w</code>和<code>x</code>，是整个计算图的根基。之所以用叶子节点的概念，是为了减少内存，<strong>在反向传播结束之后，非叶子节点的梯度会被释放掉</strong>，我们依然用上面的例子解释：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">w = torch.tensor([<span class="hljs-number">1.</span>],requires_grad = <span class="hljs-literal">True</span>)<br>x = torch.tensor([<span class="hljs-number">2.</span>],requires_grad = <span class="hljs-literal">True</span>)<br><br>a = w+x<br>b = w+<span class="hljs-number">1</span><br>y = a*b<br><br>y.backward()<br><span class="hljs-built_in">print</span>(w.is_leaf,x.is_leaf,a.is_leaf,b.is_leaf,y.is_leaf)<br><span class="hljs-built_in">print</span>(w.grad,x.grad,a.grad,b.grad,y.grad)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">输出：</span><br><span class="hljs-string">True True False False False</span><br><span class="hljs-string">tensor([5.]) tensor([2.]) None None None</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>可以看到只有<code>x</code>和<code>w</code>是叶子节点，然后反向传播计算完梯度后（<code>.backward()</code>之后），只有叶子节点的梯度保存下来了。</p><p>当然也可以通过<code>.retain_grad()</code>来保留非任意节点的梯度值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">w = torch.tensor([<span class="hljs-number">1.</span>],requires_grad = <span class="hljs-literal">True</span>)<br>x = torch.tensor([<span class="hljs-number">2.</span>],requires_grad = <span class="hljs-literal">True</span>)<br><br>a = w+x<br>a.retain_grad()<br>b = w+<span class="hljs-number">1</span><br>y = a*b<br><br>y.backward()<br><span class="hljs-built_in">print</span>(w.is_leaf,x.is_leaf,a.is_leaf,b.is_leaf,y.is_leaf)<br><span class="hljs-built_in">print</span>(w.grad,x.grad,a.grad,b.grad,y.grad)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">输出：</span><br><span class="hljs-string">True True False False False</span><br><span class="hljs-string">tensor([5.]) tensor([2.]) tensor([2.]) None None</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>torch.tensor有一个属性<code>grad_fn</code>,<code>grad_fn</code>的作用是记录创建该张量时所用的函数，这个属性反向传播的时候会用到。例如在上面的例子中，<code>y.grad_fn=MulBackward0</code>,表示y是通过乘法得到的。所以求导的时候就是用乘法的求导法则。同样的，<code>a.grad=AddBackward0</code>表示a是通过加法得到的，使用加法的求导法则。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">w = torch.tensor([<span class="hljs-number">1.</span>],requires_grad = <span class="hljs-literal">True</span>)<br>x = torch.tensor([<span class="hljs-number">2.</span>],requires_grad = <span class="hljs-literal">True</span>)<br><br>a = w+x<br>a.retain_grad()<br>b = w+<span class="hljs-number">1</span><br>y = a*b<br><br>y.backward()<br><span class="hljs-built_in">print</span>(y.grad_fn)<br><span class="hljs-built_in">print</span>(a.grad_fn)<br><span class="hljs-built_in">print</span>(w.grad_fn)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">&lt;MulBackward0 object at 0x7f95016326d8&gt;</span><br><span class="hljs-string">&lt;AddBackward0 object at 0x7f96b832d3c8&gt;</span><br><span class="hljs-string">None</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h2 id="结尾"><a href="#结尾" class="headerlink" title=" 结尾 "></a><center> 结尾 </center></h2><p>本文章摘抄于头歌教学平台中深度学习课程中静态图动态图的设计一节。</p><p>觉得写的很好，故保留下来用于复习。</p>]]></content>
    
    
    <categories>
      
      <category>DeepLearning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>东门之杨</title>
    <link href="/2023/12/03/%E4%B8%9C%E9%97%A8%E4%B9%8B%E6%9D%A8/"/>
    <url>/2023/12/03/%E4%B8%9C%E9%97%A8%E4%B9%8B%E6%9D%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="原文"><a href="#原文" class="headerlink" title=" 原文"></a><center> 原文</h2><center> 东门之杨 </center><br /><center>诗经·国风·陈风〔先秦〕</center><br /><center>东门之杨，其叶牂牂。昏以为期，明星煌煌。</center><br /><center>东门之杨，其叶肺肺。昏以为期，明星晢晢。</center><h2 id="注释"><a href="#注释" class="headerlink" title=" 注释"></a><center> 注释</h2><p>牂（zāng）牂：风吹树叶的响声。一说枝叶茂盛的样子。</p><p>昏：黄昏。期：约定的时间。</p><p>明星：明亮的星星。一说启明星，晨见东方。</p><p>煌煌：明亮的样子。</p><p>肺（pèi）肺：枝叶茂盛的样子。</p><p>晢（zhé）晢：明亮的样子</p><h2 id="译文"><a href="#译文" class="headerlink" title=" 译文"></a><center> 译文</h2><p>　　我依偎着东城门外小白杨，浓密叶片辉映着金色夕阳。约好黄昏时相会在老地方，却让我苦等到明星闪闪亮。</p><p>　　我来到东城门外白杨林边，晚霞映红了白杨浓密叶片。明明和人家约好黄昏见面，却让我苦等到星星嵌满天。</p><h2 id="赏析"><a href="#赏析" class="headerlink" title=" 赏析"></a><center> 赏析</h2><p>　　这首诗中那在白杨树下踯躅的人儿，究竟是男、是女，很难判断，但有一点可以肯定：他（或她）一定是早早吃罢晚饭，就喜孜孜来到城东门外赴约了。这约会在初恋者的心上，既隐秘又新奇，其间涌动着的，当然还有几分羞涩、几分兴奋。陈国都城的“东门”外，又正是男女青年的聚会之处，那里有“丘”、有“池”、有“枌”（白榆），“陈风”中的爱情之歌《东门之池》、《宛丘》、《月出》、《东门之枌》，大抵都产生于这块爱情圣地。</p><p>　　此时主人公的伫足之处，正有一排挺拔高耸的白杨。诗中描述它们“其叶牂牂”、“其叶肺肺”，可见正当叶儿繁茂、清碧满树的夏令。当黄昏降临、星月在天的夜晚，乌蓝的天空撒下银白的光雾，白杨树下便该映漾出一片怎样摇曳多姿的树影。清风吹过，满树的叶儿便“牂牂”、“肺肺”作响。这情景在等候情人的主人公眼中，起初一定是异常美妙的。故诗之入笔，即从黄昏夏夜中的白杨写起，表现着一种如梦如幻的画境；再加上“牂牂”、“肺肺”的树声，听来简直就是心儿的浅唱低回。</p><p>　　但当主人公久待情人而不见的时候，诗情便出现了巨大的逆转。“昏以为期，明星煌煌”、“昏以为期，明星晢晢”——字面的景象似乎依然很美，那“煌煌”、“晢晢”的启明星，高高升起于青碧如洗的夜空，静谧的世界便全被这灿烂的星辰照耀了。然而，约会的时间明明是在黄昏，此时却已是斗转星移的清寂凌晨，连启明星都已闪耀在东天，情人却不知在哪儿。诗讲究含蓄，故句面上始终未出现不见情人的字眼。但那久待的焦灼，失望的懊恼，分明已充溢于字里行间。于是“煌煌”闪烁的“明星”，似也感受了“昏以为期”的失约，而变得焦灼不安了；就是那曾经唱着歌儿似的白杨树声，也化成了一片嘘唏和叹息。</p><p>　　此诗运用的并非“兴”语，而是情景如画的“赋”法描摹。在终夜难耐的等待之中，借白杨树声和“煌煌”明星之景的点染，来烘托不见伊人的焦灼和惆怅，无一句情语，而懊恼、哀伤之情自现。这正是此诗情感抒写上的妙处。由于开笔一无征兆，直至结句方才暗示期会有失，更使诗中的景物描摹，带有了伴随情感逆转而改观的不同色彩，造成了似乐还哀的氛围递换、变化的效果。</p><h2 id="思考"><a href="#思考" class="headerlink" title=" 思考"></a><center> 思考</h2><p>这首诗的译文感觉特别有文采，浓密叶片辉映着金色夕阳、晚霞映红了白杨浓密叶片，对应着其叶牂牂和其叶肺肺，并不是直直的翻译。在脑海中会自动补齐画面~</p><p>等到多久才会来。</p><p>资料来自网站:<a href="https://so.gushiwen.cn/shiwenv_7d2ad7a0bb22.aspx">https://so.gushiwen.cn/shiwenv_7d2ad7a0bb22.aspx</a></p>]]></content>
    
    
    <categories>
      
      <category>Poem</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Poem</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数模心得</title>
    <link href="/2023/11/23/%E6%95%B0%E6%A8%A1%E5%BF%83%E5%BE%97/"/>
    <url>/2023/11/23/%E6%95%B0%E6%A8%A1%E5%BF%83%E5%BE%97/</url>
    
    <content type="html"><![CDATA[<center> 数模心得 </center><h2 id="前言"><a href="#前言" class="headerlink" title=" 前言"></a><center> 前言</h2><p>在大学一共和队友参加了四次数模，从今年过年开始的MathorCup大数据，到中旬的电工杯，到万众瞩目的国赛，最后是前几天结束的数维杯。走过很多坑，也有很多收获，在这里总结一下吧。</p><p>所有的结果：<br><img src="/../pic/mathModeling/mathModeling1.png" alt="比赛结果"></p><h2 id="前三次比赛的经历"><a href="#前三次比赛的经历" class="headerlink" title=" 前三次比赛的经历"></a><center> 前三次比赛的经历</h2><h3 id="MathorCup大数据"><a href="#MathorCup大数据" class="headerlink" title=" MathorCup大数据"></a><center> MathorCup大数据</h3><p>前三个比赛只有MathorCup获得了一个三等奖，另外两个属于是完全没有得奖。在第一次参加完MathorCup后我其实都没想过会得奖，里面一个小小的创新也是似对非对得，或许是运气好了点~。不过那段寒假时间和队友一起打着电话，在家里用着家里面的电视当屏幕用，躺在沙发上思考的日子还是有一点怀念，毕竟是第一次做数模也不知道怎么才能做好，一切都是属于尝试。</p><h3 id="电工杯"><a href="#电工杯" class="headerlink" title=" 电工杯"></a><center> 电工杯</h3><p>到了中旬，朋友说有个电工杯，叫上我的两个好队友就去参加，那段时间是在图书馆的顶楼，我们悄悄的讨论（中间还出去吃了一次火锅，态度不太端正哈哈），最后的评价指标也是使用两个方法凑合起来整成个新方法，不过电工杯也忽略了很多细节，没有过多仔细的检查，论点也不是特别有道理，论文整体也是较为混乱的。不出意外的没了。</p><h3 id="国赛"><a href="#国赛" class="headerlink" title=" 国赛"></a><center> 国赛</h3><p>为了国赛还特地的准备了一段时间，都想着在国赛的时候大展身手拿一次好的奖，大学中就可以不用再参加数学建模的比赛了。在比赛前似乎该准备的都准备了，我们选择的是B题，也就是纯数学题，没有选择大数据分析是觉得人选的太多了，而且创新起来很困难。这也是我们队第一次尝试做B题，在此之前也做过往年的题练练手，只不过做不来哈哈，折磨了很久。国赛我们是在6408做的，到了国赛那天晚上6点，我紧张的打开了题目一看，哦豁，没看懂（不过当天晚上就和队友讨论并理解了题目），当晚我们已经觉得第一问很有思路而且很简单了。以为一切都正常发展的时候，意外也就悄悄地降临。第二问花费了巨多的时间，而且由于指导老师说我们对题目理解有问题导致我们把第一问和第二问都重新花时间思考和重做了很久（最后还是改回去了），不过指导老师还是给我启发了一些思路的。</p><p>结果到前一天晚上，由于我们拖拖拉拉不断修改之前的答案，论文起笔时间夜晚，那一天晚上我们才开始做第三题。经历了一个通宵（我睡了1个小时，另一个队友只睡了半个小时），总算是有思路了，不得不说晚上的效率是真的低。第二天早上赶紧好好书写了一遍交给了写论文的同学开始誊抄，我们又开始写第四问，没想到的是第四问对我们来说有点过于困难甚至连思路都难确定，到处找找到处看看总算是混出来个像答案的东西（肯定是错的），不过已经来不及了，写论文的同学赶紧整完后，时间更是所剩无几。这个时候我们才开始写其他的摘要啊、文献什么的，由于写论文的同学前面字数不太够，我们还需要去补充字数。图片不够又到处去找图片，真的是超级混乱，在比赛前半个小时总算是像个样子了，赶紧去找老师准备好提交，当时脑子真的巨混但是特别紧张清醒。</p><p>到处再看了看也不是很仔细，最后看着只有几分钟了就赶紧提交，那天晚上才真的是赶紧我就是肖申克监狱里面的安迪他逃出监狱的感觉（有点夸张哈哈哈），之后就回家休息了一天又来上课了。</p><p>不出意外，第二天我没事看论文的时候发现了巨大的排版错误，第一问的答案的表格居然飞到别的地方了，跟表的名字完全不对应，我就知道已经完蛋了。本来后面的问就是有点问题的，就想靠前面拿分，前面结果还发生了意外，更不要说后面图片大小不一致等一些小小的问题。</p><p>结果是不出意外的国赛没了，这次失败给了我巨大的打击，感觉自己不是做数模的料，不过真的真的需要一次证明自己！继续就拉上队友又报名了数维杯。</p><h3 id="数维杯"><a href="#数维杯" class="headerlink" title=" 数维杯"></a><center> 数维杯</h3><p>又回到了6408开始这次的比赛，这次的数模可以说是这几次以来最好的一次了（选择的数据分析类题目），我们在前一天就写完了题目，用了大半天的时间和队友一点点修正论文，检查图片，检查有没有论证不清晰有问题的地方。对数学公式也较为严谨的推到过，而且代码基本上都是自己搞的，没有用统计学软件。</p><p>检查了很多问题，真的感觉完美的不能再完美了，图片我也让画图的同学画了又画，就是为了追求那完美，不能有一点点的问题出现。不过失误的是我们低估了翻译的时间，最后忙到凌晨5点提交了论文，当时脑子也不清醒，只希望翻译的没什么问题，自己也来不及去检查。最后才发现只有表格有一点点的小问题，其他都还不错的呢。</p><p>希望数维杯不要再没了，一定要证明我们队可以做好一次。</p><h2 id="结尾"><a href="#结尾" class="headerlink" title=" 结尾"></a><center> 结尾</h2><p>如果数维杯没G，那就去写一个教程~</p>]]></content>
    
    
    <categories>
      
      <category>Experience</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math modeling</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AlexNet实现</title>
    <link href="/2023/11/06/AlexNet%E5%AE%9E%E7%8E%B0/"/>
    <url>/2023/11/06/AlexNet%E5%AE%9E%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<center>AlexNet实现</center><h2 id="前言"><a href="#前言" class="headerlink" title=" 前言"></a><center> 前言</h2><p>这是在两年前接触的第一个CNN，当时也是糊里糊涂的理解。去年读过了AlexNet源论文，第一次好好推导了一遍，过后也复现了一次，不过没有好好保存，后面重装电脑无意中删掉了。</p><p>刚好要给大一学弟做培训，就打算来再做做AlexNet，也当是给之前的记录补上了。</p><h2 id="Alex-结构"><a href="#Alex-结构" class="headerlink" title=" Alex 结构"></a><center> Alex 结构</h2><p>AlexNet结构十分简单，仅用到了卷积、池化、全连接。<br><img src="/../pic/AlexNet/AlexNet1.png" alt="pipeline"><br>卷积对尺寸的变化是W &#x3D; (W - K + 2P) &#x2F; S + 1</p><p>池化对尺寸的变化是H &#x3D; (H + K) &#x2F; S + 1</p><p><strong>第一要注意原文中使用双GPU训练，故自己跑的话图像深度缩小一半</strong></p><p>下图很好说明了参数和尺寸的变化<br><img src="/../pic/AlexNet/AlexNet2.png" alt="尺寸变化"></p><p><strong>第二的注意的点是原文中尺寸是227，但实际操作时应为224才是正确的</strong></p><p>接下来就可以开始准备搭建了</p><h2 id="实现AlexNet"><a href="#实现AlexNet" class="headerlink" title=" 实现AlexNet"></a><center> 实现AlexNet</h2><h3 id="Create-The-Model"><a href="#Create-The-Model" class="headerlink" title=" Create The Model"></a><center> Create The Model</h3><p>首先导入pytorch需要的库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torchsummary <span class="hljs-keyword">import</span> summary<br></code></pre></td></tr></table></figure><p>summary可选，用来输出网络</p><p>接下来开始创建AlexNet，开始根据上面的参数设置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">AlexNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_classes=<span class="hljs-number">2</span></span>):<br>        <span class="hljs-built_in">super</span>(AlexNet, self).__init__()<br>        self.features = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">48</span>, kernel_size=<span class="hljs-number">11</span>, stride=<span class="hljs-number">4</span>, padding=<span class="hljs-number">2</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br><br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br><br>            nn.Conv2d(<span class="hljs-number">48</span>, <span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br><br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br><br>            nn.Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br><br>            nn.Conv2d(<span class="hljs-number">192</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br><br>            nn.Conv2d(<span class="hljs-number">192</span>, <span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br><br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>        )<br>        self.classifier = nn.Sequential(<br>            nn.Dropout(<span class="hljs-number">0.5</span>),<br>            nn.Linear(<span class="hljs-number">128</span> * <span class="hljs-number">6</span> * <span class="hljs-number">6</span>, <span class="hljs-number">4096</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br><br>            nn.Dropout(<span class="hljs-number">0.5</span>),<br>            nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            <br>            nn.Linear(<span class="hljs-number">4096</span>, num_classes),<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.features(x)<br>        x = torch.flatten(x, <span class="hljs-number">1</span>)<br>        x = self.classifier(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><p>这个类中创建两个函数</p><ul><li>在<code>__init__</code>中定义了2个属性，分别是features、classifier，用于卷积和全连接，在里面使用了nn.Sequential创建。</li><li>在forward中连接了网络结构</li></ul><p>继承了nn.Module类：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">super</span><span class="hljs-params">(AlexNet, self)</span></span>.<span class="hljs-built_in">__init__</span>()<br></code></pre></td></tr></table></figure><p>把矩阵展长成一维的：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">x</span> = torch.flatten(x, <span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>更具体一些相信没什么问题了，这里不过多阐述。</p><h3 id="Prepare-The-Dataset"><a href="#Prepare-The-Dataset" class="headerlink" title=" Prepare The Dataset"></a><center> Prepare The Dataset</h3><p>包含加载数据和预处理，在dog_cat文件夹下存放有train、val文件夹，里面分别有dog、cat文件夹用于存放猫和狗图片。这里给出一个别人的百度网盘的连接可以用于下载<a href="https://pan.baidu.com/share/init?surl=UOJUi-Wm6w0D7JGQduq7Ow">百度网盘猫狗数据集下载链接</a>， 密码：485q，来自<a href="https://blog.csdn.net/weixin_45836809/article/details/121690604">csdn博客</a></p><p>下面是代码，这个写法比较整洁，当然也可以分开创建，在数据处理中并没有过多的处理，如果想要有其他的变化在<code>transforms.Compose</code>中加入就可以了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets, transforms<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br><span class="hljs-comment"># 图片预处理</span><br>data_transforms = &#123;<br>    <span class="hljs-string">&#x27;train&#x27;</span>: transforms.Compose([<br>        transforms.RandomResizedCrop(<span class="hljs-number">224</span>),<br>        transforms.RandomHorizontalFlip(),<br>        transforms.ToTensor(),<br>        transforms.Normalize([<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>], [<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>])<br>    ]),<br>    <span class="hljs-string">&#x27;val&#x27;</span>: transforms.Compose([<br>        transforms.Resize(<span class="hljs-number">256</span>),<br>        transforms.CenterCrop(<span class="hljs-number">224</span>),<br>        transforms.ToTensor(),<br>        transforms.Normalize([<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>], [<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>])<br>    ]),<br>&#125;<br><br><span class="hljs-comment"># 加载数据</span><br>image_datasets = &#123;x: datasets.ImageFolder(<span class="hljs-string">&quot;/mnt/e/Dataset/dog_cat/&quot;</span>+x, data_transforms[x]) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;val&#x27;</span>]&#125;<br>dataloaders = &#123;x: DataLoader(image_datasets[x], batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">2</span>) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;val&#x27;</span>]&#125;<br></code></pre></td></tr></table></figure><p>这样写有几个好处：</p><ul><li>因为数据集的格式我们不用自定义data_loader类，当然如果不嫌麻烦也可以自己写一个</li><li>代码在使用时十分的整洁，在加载时需要指定<code> dataloaders[phase]</code>中的<code>phase</code></li></ul><p>当然如果不是很清楚其中的变量打印出来看看就明白很多了。</p><h3 id="Start-Train"><a href="#Start-Train" class="headerlink" title=" Start Train"></a><center> Start Train</h3><p>在这里只保存了最后一次训练的结果，如果想更完善一些可以对比每一次<code>epoch</code>的精度来选择性的保存结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br>device = torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>num_epochs = <span class="hljs-number">5</span><br><br>model = AlexNet().to(device)<br>criterion = nn.CrossEntropyLoss()<br>optimizer = optim.SGD(model.parameters(), lr=<span class="hljs-number">0.001</span>, momentum=<span class="hljs-number">0.9</span>)<br><br><span class="hljs-comment"># 训练模型</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;-----&quot;</span>)<br>    <span class="hljs-keyword">for</span> phase <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;val&#x27;</span>]:<br>        <span class="hljs-keyword">if</span> phase == <span class="hljs-string">&#x27;train&#x27;</span>:<br>            model.train()<br>        <span class="hljs-keyword">else</span>:<br>            model.<span class="hljs-built_in">eval</span>()<br><br>        running_loss = <span class="hljs-number">0.0</span><br>        running_corrects = <span class="hljs-number">0</span><br><br>        <span class="hljs-keyword">for</span> inputs, labels <span class="hljs-keyword">in</span> dataloaders[phase]:<br>            inputs, labels = inputs.to(device), labels.to(device)<br>            optimizer.zero_grad()<br><br>            <span class="hljs-keyword">with</span> torch.set_grad_enabled(phase == <span class="hljs-string">&#x27;train&#x27;</span>):<br>                outputs = model(inputs)<br>                _, preds = torch.<span class="hljs-built_in">max</span>(outputs, <span class="hljs-number">1</span>)<br>                loss = criterion(outputs, labels)<br><br>                <span class="hljs-keyword">if</span> phase == <span class="hljs-string">&#x27;train&#x27;</span>:<br>                    loss.backward()<br>                    optimizer.step()<br><br>            running_loss += loss.item() * inputs.size(<span class="hljs-number">0</span>)<br>            running_corrects += torch.<span class="hljs-built_in">sum</span>(preds == labels.data)<br><br>        epoch_loss = running_loss / <span class="hljs-built_in">len</span>(image_datasets[phase])<br>        epoch_acc = running_corrects.double() / <span class="hljs-built_in">len</span>(image_datasets[phase])<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&#123;&#125; Loss: &#123;:.4f&#125; Acc: &#123;:.4f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(phase, epoch_loss, epoch_acc))<br><br><span class="hljs-comment"># 保存模型</span><br>torch.save(model.state_dict(), <span class="hljs-string">&quot;model.pth&quot;</span>)<br></code></pre></td></tr></table></figure><p>在这段代码中可能需要解释的主要是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> torch.set_grad_enabled(phase == <span class="hljs-string">&#x27;train&#x27;</span>):<br></code></pre></td></tr></table></figure><p>上述代码是用于控制是否计算和存储梯度。这个语句是一个上下文管理器，它会根据参数 <code>mode</code> 来启用或禁用梯度计算。</p><p>这种做法常见于机器学习模型的训练和验证阶段。在训练阶段，我们需要计算梯度以更新模型的参数；而在验证阶段，我们通常不需要计算梯度，因此可以禁用梯度计算以节省内存。</p><p>其他的地方无非就是计算损失<code>criterion(outputs, labels)</code>、计算梯度<code>loss.backward()</code>、更新参数<code>optimizer.step()</code>。记得在开始的时候清空梯度<code>optimizer.zero_grad()</code>。</p><h3 id="Start-Predict"><a href="#Start-Predict" class="headerlink" title=" Start Predict"></a><center> Start Predict</h3><p>经过刚才的步骤会得到一个<code>model.pth</code>这个文件，接下来我们将预测单个模型，下述代码的前部分是预测，后半部分是可视化，有关预测和可视化的地方也没什么好说了，看一看就能明白。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>model = AlexNet()<br>model.load_state_dict(torch.load(<span class="hljs-string">&quot;model.pth&quot;</span>))<br>model.<span class="hljs-built_in">eval</span>()<br><br>image = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;test_data/cat1.jpg&quot;</span>)<br><br>preprocess = transforms.Compose([<br>    transforms.Resize(<span class="hljs-number">256</span>),<br>    transforms.CenterCrop(<span class="hljs-number">224</span>),<br>    transforms.ToTensor(),<br>    transforms.Normalize(mean=[<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>], std=[<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>]),<br>])<br>input_tensor = preprocess(image)<br>input_batch = input_tensor.unsqueeze(<span class="hljs-number">0</span>)<br><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    output = model(input_batch)<br><br>probabilities = F.softmax(output, dim=<span class="hljs-number">1</span>)<br><br>fig, axs = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">5</span>))<br><br>axs[<span class="hljs-number">0</span>].imshow(image)<br>axs[<span class="hljs-number">0</span>].set_title(<span class="hljs-string">&#x27;Original Image&#x27;</span>)<br><br>labels = [<span class="hljs-string">&#x27;Cat&#x27;</span>, <span class="hljs-string">&#x27;Dog&#x27;</span>]<br>axs[<span class="hljs-number">1</span>].bar(labels, probabilities.detach().numpy().flatten())<br>axs[<span class="hljs-number">1</span>].set_title(<span class="hljs-string">&#x27;Probabilities Histogram&#x27;</span>)<br><br>plt.savefig(<span class="hljs-string">&quot;reuslt.png&quot;</span>)<br></code></pre></td></tr></table></figure><p>最后的效果：<br><img src="/../pic/AlexNet/AlexNet3.png" alt="结果"><br>这里训练的比较少，效果也只能说将就，至少看起来还是有点像样子的。</p><center>——————到这里就完成了——————</center><h2 id="总结"><a href="#总结" class="headerlink" title=" 总结"></a><center> 总结</h2><p>按照上面的步骤不出意外应该是没有问题的，很适合很久没看或者是新手用来复习和学习，也算是给我之前的学习做一个小总结。</p><p>希望能帮助到你，有问题可以联系邮箱：<a href="mailto:&#x6a;&#105;&#x61;&#x68;&#x68;&#104;&#97;&#x6f;&#x40;&#x6f;&#117;&#x74;&#108;&#111;&#x6f;&#x6b;&#x2e;&#x63;&#x6f;&#109;">&#x6a;&#105;&#x61;&#x68;&#x68;&#104;&#97;&#x6f;&#x40;&#x6f;&#117;&#x74;&#108;&#111;&#x6f;&#x6b;&#x2e;&#x63;&#x6f;&#109;</a>，或者在关于中用其他的联系方式，我会尽快回复你，当然如果问题网上能查到最好自己解决~</p>]]></content>
    
    
    <categories>
      
      <category>Computer Vision</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CV</tag>
      
      <tag>AlexNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>西江月·世事短如春梦</title>
    <link href="/2023/10/31/%E8%A5%BF%E6%B1%9F%E6%9C%88%C2%B7%E4%B8%96%E4%BA%8B%E7%9F%AD%E5%A6%82%E6%98%A5%E6%A2%A6/"/>
    <url>/2023/10/31/%E8%A5%BF%E6%B1%9F%E6%9C%88%C2%B7%E4%B8%96%E4%BA%8B%E7%9F%AD%E5%A6%82%E6%98%A5%E6%A2%A6/</url>
    
    <content type="html"><![CDATA[<h2 id="原文"><a href="#原文" class="headerlink" title=" 原文"></a><center> 原文</h2><center> 西江月·世事短如春梦 </center><br /><center>朱敦儒〔宋代〕</center><br /><center>世事短如春梦，人情薄似秋云。不须计较苦劳心，万事原来有命。</center>　　<center><font color=IndianRed>幸遇三杯酒好，况逢一朵花新。</font>片时欢笑且相亲，明日阴晴未定。</center><h2 id="注释"><a href="#注释" class="headerlink" title=" 注释"></a><center> 注释</h2><p>　　西江月：原为唐教坊曲，后用作词调。《乐章集》《张子野词》并入“中吕宫”。五十字，上下片各两平韵，结句各叶一仄韵。</p><p>　　计较：算计。</p><p>　　且：姑且，聊且。</p><p>　　相亲：互相亲爱。</p><h2 id="译文"><a href="#译文" class="headerlink" title=" 译文"></a><center> 译文</h2><p>　　世事短暂，如春梦一般转瞬即逝。人情淡薄，就如秋天朗空上的薄云。不要计较自己的辛勤劳苦，万事本来已命中注定。</p><p>　　有幸遇到三杯美酒，又邂逅了一朵含苞初放的鲜花。短暂的欢乐相聚是如此的亲切，至于明天会怎么样谁也不知道了。</p><h2 id="赏析"><a href="#赏析" class="headerlink" title=" 赏析"></a><center> 赏析</h2><p>　　这首小词从慨叹人生短暂入笔，表现了词人暮年对世情的一种“彻悟”。</p><p>　　回首平生，少年的欢情，壮年的襟抱早已成为遥远的过去，飞逝的岁月在这位年迈的词人心中留下的只有世态炎凉命途多舛的凄黯记忆。所以词的起首二句“世事短如春梦，人情薄似秋云”，是饱含辛酸的笔触。这两句属对工畅，集中地、形象地表达了作者对人生的认识。“短如春梦”、“薄似秋云”的比喻熨帖而自然。接下来，笔锋一转，把世事人情的种种变化与表现归结为“命”（命运）的力量。“原来”二字，透露出一种无可如何的神情，又隐含几分激愤。在强大的命运之神面前他感到无能为力，于是消极地放弃了抗争:“不须计较苦劳心”，语气间含有对自己早年追求的悔意和自嘲。这两句倒装，不只是为了照顾押韵，也有把意思的重点落在下句的因素。情调由沉重到轻松，也反映了他从顿悟中得到解脱的心情。</p><p>　　似乎是从宿命的解释中真的得到了解脱，词人转而及时行乐，沉迷于美酒鲜花之中“幸遇三杯美酒，况逢一朵花新”，使本词转灰暗向光明、化伤悲为可喜。人之一生虽然有充满变量且难以掌握的“命”存在，但仍有己力能够操控者，譬如：面对美酒，可以独自小酌，也可偕友对饮；而目睹一朵清新可爱、初初绽放的小花，也足以兴发美感，使身心愉悦。此处词人所拣取之“酒”与“花”（“酒”、“花”，在朱词出现的频率颇高，例如：“携酒提篮，……索共梅花笑”（〈点绛唇〉）；“落帽酒中有趣，……花影阑干人静”（〈西江月〉）；“酥点梅花瘦。金杯酒”（〈点绛唇〉）……等等）颇耐人寻味，因为酒代表纵放恣肆，而花则关涉宁静自得，在深谙世事人情的无奈后，心灵自由放松了，这两种不同的生命情境便能兼而有之。朱敦儒这种通过达命而产生的欢喜态度，后出的张孝祥（一一三三～一一七○）领会亦深，因此填有“世路如今已惯，此心到处悠然。寒光亭下水连天，飞起沙鸥一片”。</p><p>　　上下文都是议论，使得这属对工巧的两句尤其显得清新有趣。着墨不多，主人公那种得乐且乐的生活情态活脱脱地展现出来。结语两句，虽以“片时欢笑且相亲”自安自慰，然而至于“明日阴晴未定”，则又是天道无常，陷入更深的叹息中了。“且”是“姑且”、“聊且”的意思。“阴晴未定是感叹世事的翻覆无定，或许还有政治上的寓意。下片末句与上片“万事原来有命”呼应，又回到“命”上去了。作者的生活态度是强作达观而实则颓唐。</p><p>　　起首二句是饱含辛酸的笔触，形象地表达了作者对人生的认识。接下来，笔锋一转，把世事人情的种种变化与表现归结为“命”的力量。结语两句，则又是天道无常，陷入更深的叹息。这首词对仗工整，比喻熨贴而自然，自然流转，若不经意，全词如骏马注坡，一气直下，上下文的议论，亦使得对应句尤其清新有趣</p><h2 id="思考"><a href="#思考" class="headerlink" title=" 思考"></a><center> 思考</h2><p>　　从诗人的角度看来人间世事短、人情薄，而且万事有命这种躺平的态度也挺适合现在社会；不如及时行乐，好酒、鲜花，与朋友的相聚已经足够打发今日，至于明天怎样，谁又管得着呢。</p><p>　　古代的躺平态度在现在浮躁的社会有时候能起到作用，客观的来讲还是需要分门别类，毕竟有时候我还是希望自己能努力努力去更好的地方，有自己想完成的事。</p><p>　　不过这首诗闲暇时光来看还是非常不错的！</p><hr><p>资料来自网站: <a href="https://so.gushiwen.cn/shiwenv_68b2bf1dbf8b.aspx">https://so.gushiwen.cn/shiwenv_68b2bf1dbf8b.aspx</a></p>]]></content>
    
    
    <categories>
      
      <category>Poem</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Poem</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>记录一些好用的网站</title>
    <link href="/2023/10/30/%E8%AE%B0%E5%BD%95%E4%B8%80%E4%BA%9B%E5%A5%BD%E7%94%A8%E7%9A%84%E7%BD%91%E7%AB%99/"/>
    <url>/2023/10/30/%E8%AE%B0%E5%BD%95%E4%B8%80%E4%BA%9B%E5%A5%BD%E7%94%A8%E7%9A%84%E7%BD%91%E7%AB%99/</url>
    
    <content type="html"><![CDATA[<h3 id="英语往年试题的网站"><a href="#英语往年试题的网站" class="headerlink" title="英语往年试题的网站"></a>英语往年试题的网站</h3><p><a href="https://pan.uvooc.com/Learn/CET/CET6">https://pan.uvooc.com/Learn/CET/CET6</a></p><hr><h3 id="SCI-Hub论文下载可用网址链接-实时更新"><a href="#SCI-Hub论文下载可用网址链接-实时更新" class="headerlink" title="SCI-Hub论文下载可用网址链接 - 实时更新"></a>SCI-Hub论文下载可用网址链接 - 实时更新</h3><p><a href="https://tool.yovisun.com/scihub/">https://tool.yovisun.com/scihub/</a></p><hr><h3 id="读论文工具"><a href="#读论文工具" class="headerlink" title="读论文工具"></a>读论文工具</h3><p><a href="https://readpaper.com/">https://readpaper.com/</a></p><hr><h3 id="电子书下载网站"><a href="#电子书下载网站" class="headerlink" title="电子书下载网站"></a>电子书下载网站</h3><p><a href="https://zh.annas-archive.org/">https://zh.annas-archive.org/</a></p><p>配上文件转化器（转PDF等）</p><p><a href="https://convertio.co/zh/">https://convertio.co/zh/</a></p><hr>]]></content>
    
    
    <categories>
      
      <category>Tools &amp;&amp; Website</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CET</tag>
      
      <tag>Paper</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
